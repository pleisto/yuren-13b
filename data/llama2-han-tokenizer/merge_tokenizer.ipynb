{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "from sentencepiece import sentencepiece_model_pb2 as sp_pb2_model\n",
    "import sentencepiece as spm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32001 5456\n",
      "['<s>', '</s>', '<unk>']\n",
      "[1, 2, 0]\n",
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n"
     ]
    }
   ],
   "source": [
    "llama_tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-hf\")\n",
    "han_sp_model = spm.SentencePieceProcessor()\n",
    "han_sp_model.Load(\"./hanzi.model\")\n",
    "\n",
    "llama_spm = sp_pb2_model.ModelProto()\n",
    "llama_spm.ParseFromString(llama_tokenizer.sp_model.serialized_model_proto())\n",
    "chinese_spm = sp_pb2_model.ModelProto()\n",
    "chinese_spm.ParseFromString(han_sp_model.serialized_model_proto())\n",
    "\n",
    "# print number of tokens\n",
    "print(len(llama_tokenizer),len(han_sp_model))\n",
    "print(llama_tokenizer.all_special_tokens)\n",
    "print(llama_tokenizer.all_special_ids)\n",
    "print(llama_tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37456\n",
      "Before:32000\n",
      "Skipping ▁\n",
      "New model pieces: 36843\n"
     ]
    }
   ],
   "source": [
    "## Add Chinese tokens to LLaMA tokenizer\n",
    "llama_spm_tokens_set=set(p.piece for p in llama_spm.pieces)\n",
    "print(len(llama_spm_tokens_set)+len(chinese_spm.pieces))\n",
    "print(f\"Before:{len(llama_spm_tokens_set)}\")\n",
    "for p in chinese_spm.pieces:\n",
    "    piece = p.piece\n",
    "    if piece == \"▁\":\n",
    "        print(\"Skipping ▁\")\n",
    "        continue\n",
    "    if piece not in llama_spm_tokens_set:\n",
    "        new_p = sp_pb2_model.ModelProto().SentencePiece()\n",
    "        new_p.piece = piece\n",
    "        new_p.score = 0\n",
    "        llama_spm.pieces.append(new_p)\n",
    "print(f\"New model pieces: {len(llama_spm.pieces)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./dist/tokenizer_config.json',\n",
       " './dist/special_tokens_map.json',\n",
       " './dist/tokenizer.model',\n",
       " './dist/added_tokens.json')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./han_llama.model', 'wb') as f:\n",
    "    f.write(llama_spm.SerializeToString())\n",
    "tokenizer = LlamaTokenizer(vocab_file='./han_llama.model')\n",
    "tokenizer.save_pretrained(\"./dist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized by Han-LLaMA tokenizer:{'input_ids': [1, 29871, 30374, 31266, 30564, 30662, 30675, 31679, 30275, 31611, 30275, 32809, 30356, 31358, 30963, 31057, 30909, 32173, 31174, 30855, 35496, 31412, 34105, 30910, 31599, 32799, 30257, 30210, 31474, 35761, 30855, 35496, 31412, 34105, 30392, 33545, 31174, 30275, 30356, 30607, 31424, 30690, 30705, 30210, 30486, 31074, 31867, 30392, 30528, 35930, 31180, 30910, 31599, 30210, 30908, 30698, 31359, 34753, 30392, 33545, 30846, 30672, 30356, 30753, 30806, 30886, 30494, 30564, 30437, 30888, 31349, 31424, 30690, 30705, 33185, 30356, 31195, 31424, 30622, 30685, 30502, 31047, 30470, 32818, 33652, 30895, 31062, 30210, 30908, 30698, 31074, 31180, 30573, 32173, 31174, 30855, 35496, 31412, 34105, 30910, 31599, 32799, 30257, 31424, 31302, 30544, 30847, 30557, 31474, 35761, 30287, 33266, 30988, 30698, 31376, 30651, 32045, 31830, 30606, 30374, 30594, 30690, 30275, 30356, 31141, 31085, 30564, 30437, 30888, 31349, 31579, 31522, 30573, 31084, 31943, 31947, 30752, 35937, 33201, 32257, 30210, 30685, 30802, 30257, 34992, 30648, 32727, 31695, 34852, 30275, 31376, 31174, 31041, 30732, 33266, 31359, 31268, 31366, 33646, 32292, 31835, 30753, 30806, 35937, 33201, 30374, 30910, 31599, 30687, 33239, 30666, 33237, 31901, 30886, 30374, 30910, 31599, 31168, 31655, 34686, 31074, 33545, 30846, 30528, 35930, 31180, 30910, 31599, 32727, 31695, 30564, 30437, 30888, 31349, 30461, 31632, 31412, 34105, 31264, 36550, 30525, 31331, 32727, 31695, 31977, 30502, 33996, 30413, 30846, 33592, 30666, 33237, 35496, 31420, 30461, 31632, 30705, 30545, 31032, 30705, 30356, 36477, 30705, 30287, 31151, 35496, 31427, 34459, 32784], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'新华社北京电中共中央国务院关于促进民营经济发展壮大的意见民营经济是推进中国式现代化的生力军是高质量发展的重要基础是推动我国全面建成社会主义现代化强国实现第二个百年奋斗目标的重要力量为促进民营经济发展壮大现提出如下意见一总体要求以习'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#llama_tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-hf\")\n",
    "han_llama_tokenizer = LlamaTokenizer.from_pretrained(\"./dist\")\n",
    "\n",
    "text='新华社北京电中共中央国务院关于促进民营经济发展壮大的意见民营经济是推进中国式现代化的生力军是高质量发展的重要基础是推动我国全面建成社会主义现代化强国实现第二个百年奋斗目标的重要力量为促进民营经济发展壮大现提出如下意见一总体要求以习近平新时代中国特色社会主义思想为指导深入贯彻党的二十大精神坚持稳中求进工作总基调完整准确全面贯彻新发展理念加快构建新发展格局着力推动高质量发展坚持社会主义市场经济改革方向坚持两个毫不动摇加快营造市场化法治化国际化一流营商环境'\n",
    "#la = llama_tokenizer.tokenize(text)\n",
    "max_len=115\n",
    "han_la = han_llama_tokenizer(text)\n",
    "#print(f\"Tokenized by LLaMA tokenizer:{la}\")\n",
    "print(f\"Tokenized by Han-LLaMA tokenizer:{han_la}\")\n",
    "\n",
    "\n",
    "def chunk_with_overlap(input_list,chunk_size=max_len, overlap=2):\n",
    "    assert chunk_size > overlap, \"max_len should be larger than overlap size\"\n",
    "    \n",
    "    chunks = []\n",
    "    start_index = 0\n",
    "\n",
    "    # 处理第一个chunk\n",
    "    if len(input_list) >= chunk_size:\n",
    "        chunks.append(input_list[:chunk_size])\n",
    "        start_index += chunk_size\n",
    "\n",
    "    # 当剩余的元素足够构成一个chunk时，循环继续\n",
    "    while start_index + chunk_size - overlap <= len(input_list):\n",
    "        chunk = input_list[start_index - overlap : start_index + chunk_size - overlap]\n",
    "        chunks.append(chunk)\n",
    "        start_index += chunk_size - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "\n",
    "for x in chunk_with_overlap(text):\n",
    "    display(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
